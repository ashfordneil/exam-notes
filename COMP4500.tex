\documentclass[landscape]{cheat}
\usepackage{amsmath}
\usepackage{listings}

\begin{document}
\begin{multicols*}{3}

\begin{center}
\Large{\underline{COMP4500 Cheat Sheet}} \\
\end{center}

%% START HERE
\section{Complexities}

\subsection{Definitions of Complexities}
\begin{align*}
    f(n) = \Theta(g(n)) \Leftrightarrow& \forall n > n_0, f(n) \in [c_1 \cdot g(n), c_2 \cdot g(n)] \\
    f(n) = O(g(n)) \Leftrightarrow& \forall n > n_0, f(n) \in [0, c \cdot g(n)) \\
    f(n) = \Omega(g(n)) \Leftrightarrow& \forall n > n_0, f(n) \in [c \cdot g(n), \infty) \\
    f(n) = o(g(n)) \Leftrightarrow& \lim_{n \rightarrow \infty} \frac {f(n)} {g(n)} = 0 \\
    f(n) = \omega(g(n)) \Leftrightarrow& \lim_{n \rightarrow \infty} \frac {g(n)} {f(n)} = 0 \\
\end{align*}

\subsection{Master Theorem}
\begin{description}
    \item[Find] $a \geq 1$ and $b \geq 1$ such that $T(n) = a T\left(\frac n b\right) + f(n)$
\end{description}
\begin{align*}
    n^{\log_b a} > f(n) \Rightarrow& T(n) \in \Theta(n^{log_b a}) \\
    n^{\log_b a} \cdot \lg^k(n) \in \Theta(f(n)) \Rightarrow& T(n) \in \Theta(\lg^{k+1}(n) \cdot n^{\log_b a}) \\
    f(n) > n^{\log_b a} \Rightarrow& T(n) \in \Theta(f(n)) \\
\end{align*}

\begin{description}
    \item[Regularity] (defined below) in $f(n)$ is needed for case three of the master theorem.
\end{description}
\begin{align*}
    \exists c. c < 1 \land af\left(\frac n b\right) < cf(n)
\end{align*}
\begin{description}
    \item[Polynomially Larger Than] (defined below) is used in cases 1 and 3 of the master theorem.
\end{description}
\begin{align*}
    f(n) > g(n) \Leftrightarrow& \exists \epsilon. \epsilon > 0 \land f(n) \in \Omega(g(n) \cdot n^\epsilon)
\end{align*}

\subsection{Amortized Analysis}
\begin{description}
    \item[Aggregate analysis] Evaluate the summation of multiple operations, and then take the average.
    \item[Accounting method] Define an amortized cost for each operation, that will never decrease the total cost.
        Perform analysis with this cost instead.
    \item[Potential method] Define a cost function $\Phi$ on the data structure. Find an amortized cost such that $\hat{c_i} = c_i + \Delta \Phi(D_i)$.
        Ensure that $\forall i. \Phi(D_i) \geq \Phi(D_0)$.
\end{description}

\section{Graph Errata}
\subsection{Handshake Lemma}
\begin{lstlisting}
for v in G.V
    for e in G.Adj(e)
        OPERATION
\end{lstlisting}
\begin{description}
    \item[Complexity] $\Theta((V+E) \times OPERATION)$ for adjacency list.
    \item[Complexity] $\Theta((V+E) \times OPERATION + V^2)$ for adjacency matrix.
\end{description}

\section{Graph Algorithms}

\subsection{Breadth First Search}
\begin{description}
    \item[Complexity] $\Theta(V+E)$ for adjacency list.
    \item[Complexity] $\Theta(V^2)$ for adjacency matrix.
    \item[Finds] breadth first tree of graph.
    \item[Finds] shortest path to each node from source, assuming equal weights.
\end{description}

\subsection{Depth First Search}
\begin{description}
    \item[Complexity] $\Theta(V+E)$ for adjacency list.
    \item[Complexity] $\Theta(V^2)$ for adjacency matrix.
    \item[Finds] depth first forests of graph.
    \item[Finds] loops, if you find a grey vertex in DFS-VISIT.
    \item[Finds] a topological sort, if you output in descending order by $u.finish$.
\end{description}

\subsection{Strongly Connected Components}
\begin{lstlisting}
STRONGLY-CONNECTED-COMPONENTS(G)
    call DFS(G)
    compute G^T
    call DFS(G^T) but consider vertices
        in decreasing order of u.f
    return discrete trees of second
        search
\end{lstlisting}

\columnbreak
\subsection{Kruskal's Minimum Spanning Tree}
\begin{lstlisting}
MST-KRUSKAL(G, w)
    A = {}
    for u in G.V
        MAKE-SET(u)
    SORT(G.E, w)
    for u,v in G.E
        if FIND-SET(u) != FIND-SET(v)
            A = A + {(u, v)}
            UNION(u, v)
    return A
\end{lstlisting}
\begin{description}
    \item[Complexity] $\Theta(V+E\log{E})$
    \item[Finds] Minimum spanning tree - subset of edges such that all vertices are connected and sum of edge weight is minimised.
\end{description}

\subsection{Priority First Search}
\begin{lstlisting}
PRIORITY-FIRST-SEARCH(G, s, ...)
    INIT-SINGLE-SOURCE(G, s)
    Q = G.V
    while Q != {}
        u = EXTRACT-MIN(Q)
        for v in G.Adj(u)
            RELAX(u, v, ...)
\end{lstlisting}
\begin{description}
    \item[Complexity] $\Theta(V)\cdot T_{EXTRACT} + \Theta(E)\cdot T_{DECREASE}$
    \item[Complexity] $O(E\log{V})$ for binary heap.
    \item[Complexity] $O(E+V\log{V})$ for fibonacci heap.
\end{description}

\subsection{Prim's Minimum Spanning Tree}
Priority first search with
\lstinline{P(u, v, w) = w(u, v)}.
\begin{description}
    \item[Complexity] Same as priority first search.
    \item[Finds] Minimum spanning tree.
\end{description}

\subsection{Dijkstra'a Algorithm}
Priority first search with \lstinline{P(u, v, w) = u.key + w(u, v)}.
\begin{description}
    \item[Complexity] Same as priority first search.
    \item[Finds] Single source shortest path to any node, assuming no negative edges.
\end{description}

\subsection{Bellman Ford}
\begin{lstlisting}
BELLMAN-FORD(G, w, s)
    INIT-SINGLE-SOURCE(G, s)
    for i in 1..|G.V|-1
        for u,v in G.E
            RELAX(u, v, w)
    for u,v in G.E
        if v.d > u.d + w(u,v)
            return FALSE
    return TRUE
\end{lstlisting}
\begin{description}
    \item[Complexity] $O(V\cdot E)$
    \item[Finds] Single source shortest path to any node, assuming no negative cycles.
    \item[Finds] The existence of negative cycles.
\end{description}

\section{Dynamic Programming}
\begin{description}
    \item[Caching] of solutions to subproblems in typically recursive algorithms.
    \item[Top down] is still recursive, but a cache of solved subproblems is maintained.
    \item[Buttom up] is iterative, starting with the base case and building constructively up to the original problem.
        Results are stored in arrays, and better constant factors are achieved.
\end{description}

\section{P vs NP}
\begin{description}
    \item[P] problems are those that can be solved in polynomial time by a deterministic turing machine.
    \item[NP] problems are those that can be solved in polynomial time by a nondeterministic turing machine.
        Also the problems that a solution (certificate) to which can be \emph{verified} in polynomial time by a deterministic turing machine.
    \item[NP hard] problems are problems that any problem in NP can be reduced to in polynomial time.
    \item[NP complete] problems are NP hard problems that are in NP.
    \item[P = NP] probably not.
\end{description}

\end{multicols*}
\end{document}
